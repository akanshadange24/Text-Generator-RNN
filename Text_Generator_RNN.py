# -*- coding: utf-8 -*-
"""Untitled18.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FpojHCMKhsNznBrGl-ogRoumKW8fiz3g
"""

labeled_text_data = [
    ("Buy now and get 50% off!", "ad"),
    ("This is a regular news article about the weather.", "non-ad"),
    ("Limited time offer! Don't miss out!", "ad"),
    ("The company announced its quarterly earnings today.", "non-ad"),
    ("Click here to win a free prize!", "ad"),
    ("Here is the schedule for the upcoming events.", "non-ad"),
    ("Huge discounts on all products this week!", "ad"),
    ("Learn more about our services on our website.", "ad"),
    ("This is a forum post discussing a technical issue.", "non-ad"),
    ("Sign up for our newsletter for exclusive deals.", "ad"),
    ("The capital of France is Paris.", "non-ad"),
    ("Discover amazing deals on electronics!", "ad"),
    ("The meeting minutes from the last session are available.", "non-ad"),
    ("Get yours today before they are all gone!", "ad"),
    ("This is a recipe for chocolate chip cookies.", "non-ad"),
    ("We are hiring! Apply now!", "ad"),
    ("The history of the internet is fascinating.", "non-ad"),
    ("Special offer just for you!", "ad"),
    ("Customer reviews are important for feedback.", "non-ad"),
    ("Download our app for a better experience.", "ad"),
    ("The quick brown fox jumps over the lazy dog.", "non-ad"),
    ("Earn rewards with every purchase.", "ad"),
    ("Frequently asked questions about our product.", "non-ad"),
    ("Join our community and connect with others.", "ad"),
    ("The sun is a star.", "non-ad"),
    ("Unlock exclusive benefits with our premium plan.", "ad"),
    ("Terms and conditions apply.", "non-ad"),
    ("Find the perfect gift for your loved ones.", "ad"),
    ("The Earth revolves around the sun.", "non-ad"),
    ("Save big on your next order!", "ad")
]

import torch
import torch.nn as nn

class AdRNN(nn.Module):
    def __init__(self, input_size, hidden_size):
        super(AdRNN, self).__init__()
        self.hidden_size = hidden_size
        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)
        # New classification layer for binary output
        self.fc = nn.Linear(hidden_size, 1) # One output unit for binary classification

    def forward(self, x, hidden):
        # Pass input through RNN layer
        out, hidden = self.rnn(x, hidden)
        # Pass the output of the last time step through the classification layer
        # We take the output from the last time step for classification
        out = self.fc(out[:, -1, :]) # Select output of the last time step

        # Apply sigmoid activation for binary classification (output between 0 and 1)
        out = torch.sigmoid(out)

        return out, hidden

    def init_hidden(self, batch_size):
        return torch.zeros(1, batch_size, self.hidden_size)

# Define vocab_size and hidden_size before instantiating the model
# vocab_size is the size of the vocabulary (number of unique characters)
# hidden_size is the number of features in the hidden state of the RNN
vocab_size = 0 # This will be updated later with the correct vocabulary size
hidden_size = 128 # You can adjust this value


# Instantiate the modified model
model = AdRNN(vocab_size, hidden_size)

print(model)

import torch
import torch.nn as nn

def one_hot_encode(sequence, vocab_size):
    # Create a tensor of zeros with shape (sequence_length, vocab_size)
    encoding = torch.zeros(len(sequence), vocab_size)
    # For each index in the sequence, set the corresponding element in the encoding to 1
    for i, char_index in enumerate(sequence):
        encoding[i, char_index] = 1
    return encoding

# 1. Create numerical representations of the text data and their corresponding labels
all_text = "".join([text for text, label in labeled_text_data])
chars = sorted(list(set(all_text)))
char_to_idx = {char: idx for idx, char in enumerate(chars)}
idx_to_char = {idx: char for idx, char in enumerate(chars)}

# Recalculate vocab_size based on the labeled data
vocab_size = len(chars)

encoded_labeled_data = []
for text, label in labeled_text_data:
    encoded_text = [char_to_idx[char] for char in text]
    numerical_label = 1 if label == "ad" else 0
    encoded_labeled_data.append((encoded_text, numerical_label))

# Instantiate the model again with the correct vocab size if it was created with the old one
# Assuming the model structure AdRNN is already defined and correct from previous steps
# If the model was already instantiated with the wrong vocab_size, re-instantiate it.
# If the model was not yet instantiated, this is where it should be done after defining vocab_size.
# For robustness, let's re-instantiate the model with the correct vocab_size.
model = AdRNN(vocab_size, hidden_size)


# 2. Define the loss function suitable for binary classification
criterion = nn.BCELoss()

# 3. Define the optimizer for the model parameters
lr = 0.005 # Using the same learning rate as before
optimizer = torch.optim.Adam(model.parameters(), lr=lr)

# 4. Implement the training loop
num_epochs_classification = 100 # Number of epochs for classification training

print("Starting classification model training...")

for epoch in range(num_epochs_classification):
    total_loss = 0
    for encoded_text, numerical_label in encoded_labeled_data:
        # Convert input sequence to a one-hot encoded tensor
        input_tensor = one_hot_encode(encoded_text, vocab_size).unsqueeze(0)

        # Convert numerical label to a tensor
        target_tensor = torch.tensor([numerical_label], dtype=torch.float32).unsqueeze(0)

        # Initialize the hidden state of the RNN
        hidden = model.init_hidden(1)

        # Pass the input tensor and hidden state through the model
        output, hidden = model(input_tensor, hidden)

        # Calculate the loss
        loss = criterion(output.squeeze(), target_tensor.squeeze())

        # Perform backpropagation and update weights
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        total_loss += loss.item()

    # 5. Print the loss at regular intervals
    if (epoch + 1) % 10 == 0: # Print every 10 epochs
        print(f"Epoch [{epoch+1}/{num_epochs_classification}], Loss: {total_loss/len(encoded_labeled_data):.4f}")

print("Classification model training finished.")

# 1. Check if the website_text variable exists and is not empty.
if 'website_text' in locals() and website_text:
    # 2. Set the model to evaluation mode and disable gradient calculations.
    model.eval()
    with torch.no_grad():
        # 3. Define a sequence length for inference.
        seq_length_inference = 30  # Consistent with training

        print("Analyzing scraped website text for potential ad sequences:")

        # 4. Iterate through the website_text in steps of seq_length_inference.
        for i in range(0, len(website_text) - seq_length_inference + 1, seq_length_inference):
            input_seq = website_text[i : i + seq_length_inference]

            # a. Convert the text sequence to a one-hot encoded tensor.
            # Handle characters not in the training vocabulary
            encoded_seq = [char_to_idx.get(char, 0) for char in input_seq] # Map unknown chars to index 0
            input_tensor = one_hot_encode(encoded_seq, vocab_size).unsqueeze(0)

            # b. Initialize the hidden state of the model.
            hidden = model.init_hidden(1)

            # c. Pass the input tensor and hidden state through the trained model.
            output, hidden = model(input_tensor, hidden)

            # d. Convert the model's output probability to a predicted class (0 or 1).
            predicted_class = 1 if output.squeeze().item() > 0.5 else 0

            # e. Print the predicted class for each processed sequence.
            print(f"Sequence: '{input_seq}' -> Predicted class: {predicted_class}")

else:
    print("No scraped website text available to analyze.")

from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# 1. Split the encoded_labeled_data into training and testing sets
train_data, test_data = train_test_split(encoded_labeled_data, test_size=0.2, random_state=42)

# Prepare lists to store actual and predicted labels
actual_labels = []
predicted_labels = []

print("Evaluating the model on the test set...")

# 2. Iterate through the test set
model.eval() # Set the model to evaluation mode
with torch.no_grad(): # Disable gradient calculation during inference
    for encoded_text, numerical_label in test_data:
        actual_labels.append(numerical_label)

        # Convert the encoded text sequence to a one-hot encoded tensor
        input_tensor = one_hot_encode(encoded_text, vocab_size).unsqueeze(0)

        # Initialize the hidden state of the model
        hidden = model.init_hidden(1)

        # Pass the input tensor and hidden state through the trained model
        output, hidden = model(input_tensor, hidden)

        # Convert the model's output probability to a predicted class (0 or 1)
        predicted_class = 1 if output.squeeze().item() > 0.5 else 0
        predicted_labels.append(predicted_class)

# 3. Calculate the classification metrics
accuracy = accuracy_score(actual_labels, predicted_labels)
precision = precision_score(actual_labels, predicted_labels)
recall = recall_score(actual_labels, predicted_labels)
f1 = f1_score(actual_labels, predicted_labels)

# 4. Print the calculated evaluation metrics
print("\nClassification Metrics on Test Set:")
print(f"Accuracy: {accuracy:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")
print(f"F1-score: {f1:.4f}")